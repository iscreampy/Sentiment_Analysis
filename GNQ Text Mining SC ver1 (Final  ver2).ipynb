{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vgoyal1\\AppData\\Local\\Continuum\\Anaconda27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "import nltk.classify.util\n",
    "import pandas as pd\n",
    "from pattern.en import sentiment\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize stopWords\n",
    "stopWords = []\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(stopWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    \n",
    "    #print (\"pprocesstweet\",\" \", tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "def getFeatureVector(tweet):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "stopWords = getStopWordList('stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781 sentences in training data\n"
     ]
    }
   ],
   "source": [
    "xl = pd.ExcelFile('aws_tweets_data_cleaned.xls')\n",
    "inpTweets = xl.parse(\"aws_tweets_data\")\n",
    "\n",
    "featureList = []\n",
    "features = inpTweets.iloc[:,0:]\n",
    "# Get tweet words\n",
    "training_data = []\n",
    "for row in (features.values):\n",
    "    tweet = row[0]\n",
    "    sentiment = row[1]\n",
    "    training_data.append({\"class\":sentiment, \"sentence\":tweet})\n",
    "print (\"%s sentences in training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1781 documents\n",
      "3 classes [u'positive', u'neutral', u'negative']\n",
      "1438 unique stemmed words [u'demand', u'frontp', u'symantecvid', u'spel', u'swap', u'sorry', u'spec', u'digit', u'sapphirenow', u'every']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    \n",
    "    tweet = pattern['sentence']\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Remove numbers\n",
    "    tweet = re.sub('[\\d]+', ' ', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    \n",
    "    \n",
    "    # tokenize each word in the sentence\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    tok = []\n",
    "    \n",
    "    for w in tokens:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            tok.append(w)\n",
    "     \n",
    "    #add to our words list\n",
    "    words.extend(tok)\n",
    "    # add to documents in our corpus\n",
    "    documents.append((tok, pattern['class']))\n",
    "    # add to our classes list\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = list(set(words))\n",
    "\n",
    "# remove duplicates\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 1)\n",
      "(16, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[('wow', 'positive'), ('new', 'positive'), ('announce', 'positive'), ('annoncing', 'positive'), ('hugs', 'positive'), ('excited', 'positive'), ('supercharge', 'positive'), ('strengthen', 'positive'), ('great', 'positive'), ('good', 'positive')]\n"
     ]
    }
   ],
   "source": [
    "# append input data specific adjectives\n",
    "positives = pd.read_table(\"positives.txt\")\n",
    "print (positives.shape)\n",
    "negatives = pd.read_table(\"negatives.txt\")\n",
    "print (negatives.shape)\n",
    "\n",
    "print (type(positives))\n",
    "doc=[]\n",
    "for i in range(1,len(positives)):\n",
    "    doc.append((positives.iloc[i][0],\"positive\"))\n",
    "\n",
    "for i in range(1,len(negatives)):\n",
    "    doc.append((negatives.iloc[i][0],\"negative\"))\n",
    "\n",
    "print (doc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1887 documents\n",
      "1439 words\n",
      "92 positives\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'internetofth', u'emerg', u'cws', u'sysct', 'acquires']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding positive and negative keywords\n",
    "documents.extend(doc)\n",
    "words.extend(positives)\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(words), \"words\")\n",
    "print (len(positives), \"positives\")\n",
    "words[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    training.append(bag)\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'azurehelp', u'ie', u'fail', u'javascrib', u'mvc', u'az', u'deploy', u'url']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample training/output\n",
    "i = 5\n",
    "w = documents[i][0]\n",
    "print ([stemmer.stem(word.lower()) for word in w])\n",
    "print (training[i])\n",
    "print (output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to strip u character\n",
    "STRIPUNICODEChar=0\n",
    "if STRIPUNICODEChar:\n",
    "    my_list = documents\n",
    "    upddocs=[]\n",
    "\n",
    "    for tokens in my_list:\n",
    "        asciitokens = []\n",
    "        if (len(tokens)> 1):\n",
    "            for tok in iter(tokens[0]):\n",
    "                asciitoks = tok.encode('ascii')\n",
    "                asciitokens.append(asciitoks)\n",
    "        else:\n",
    "            asciitoks = token.encode('ascii')\n",
    "            asciitokens.append(asciitoks)\n",
    "\n",
    "        upddocs.append((asciitokens, [tokens[1].encode('ascii')]))\n",
    "\n",
    "    print (len(upddocs))\n",
    "    documents = upddocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(1509L,)\n",
      "[([u'microsoft', u'azure', u'kajaani', u'lecture', u'create', u'datacenters', u'url'], u'neutral'), ([u'sample', u'may', u'th', u'delete', u'row', u'windows', u'azure', u'table', u'storage', u'without', u'retrieving', u'first', u'url', u'microsoft'], u'positive'), ([u'anybody', u'know', u'much', u'microsoft', u'azure', u'cloud', u'computing', u'cloudserver'], u'positive'), ([u'rt', u'put', u'strategy', u'internet', u'things', u'place', u'microsoft', u'azure', u'intelligent', u'system', u'service', u'url'], u'positive'), ([u'looks', u'like', u'excellent', u'sessions', u'allot', u'azure', u'devops', u'goodness', u'coming'], u'positive'), ([u'azurehelp', u'ie', u'fails', u'javascripts', u'mvc', u'azure', u'deployment', u'url'], u'negative'), ([u'moving', u'hybrid', u'cloud', u'microsoft', u'azure', u'url'], u'neutral'), ([u'update', u'microsoft', u'powershell', u'ise', u'script', u'browser', u'script', u'analyzer', u'url', u'available', u'sysctr', u'azure', u'scvmm', u'hyperv', u'etc'], u'positive'), ([u'mobile', u'app', u'backend', u'microsoft', u'azure', u'cloud', u'win', u'bluetooth', u'speaker', u'mobile', u'app', u'backend', u'wi', u'url', u'via'], u'positive'), ([u'microsoft', u'azure', u'security', u'privacy', u'compliance', u'url'], u'neutral')]\n",
      "[ [u'microsoft', u'azure', u'kajaani', u'lecture', u'create', u'datacenters', u'url']\n",
      " [u'sample', u'may', u'th', u'delete', u'row', u'windows', u'azure', u'table', u'storage', u'without', u'retrieving', u'first', u'url', u'microsoft']\n",
      " [u'anybody', u'know', u'much', u'microsoft', u'azure', u'cloud', u'computing', u'cloudserver']\n",
      " ..., 'locks' 'losing' 'issues']\n",
      "(1887L,)\n"
     ]
    }
   ],
   "source": [
    "#trial purpose only\n",
    "tdata = (np.array([x[0] for x in documents]))\n",
    "target = (np.array([x[1] for x in documents]))\n",
    "\n",
    "X_train ,x_test, Y_train, y_test = train_test_split(tdata,target,test_size=0.2)\n",
    "print (type(X_train))\n",
    "print (X_train.shape)\n",
    "print (documents[:10])\n",
    "print (tdata)\n",
    "print (tdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1887, 2)\n",
      "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "[ [u'microsoft', u'azure', u'kajaani', u'lecture', u'create', u'datacenters', u'url']\n",
      " [u'sample', u'may', u'th', u'delete', u'row', u'windows', u'azure', u'table', u'storage', u'without', u'retrieving', u'first', u'url', u'microsoft']\n",
      " [u'anybody', u'know', u'much', u'microsoft', u'azure', u'cloud', u'computing', u'cloudserver']\n",
      " ..., 'locks' 'losing' 'issues']\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(documents, columns=('text','class'))\n",
    "print (df.shape)\n",
    "tfidf_vect= TfidfVectorizer(  use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "type(df['text'])\n",
    "#X = tfidf_vect.fit_transform(str(x) for x in df['text'].values)\n",
    "X = tfidf_vect.fit_transform(raw_documents=(str(x) for x in df['text'].values))\n",
    "y = df['class'].values\n",
    "print (tfidf_vect)\n",
    "print(df['text'].values)\n",
    "\n",
    "#feature_names = tfidf_vect.get_featuree_names()\n",
    "#print(feature_names)\n",
    "\n",
    "#data_mat=pd.DataFrame(dense,columns=feature_names)\n",
    "#print(data_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1887, 2)\n",
      "(1264, 1860)\n",
      "(1264L,)\n",
      "(623, 1860)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(documents, columns=('text','class'))\n",
    "print (df.shape)\n",
    "tfidf_vect= TfidfVectorizer(  use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "type(df['text'])\n",
    "\n",
    "X = tfidf_vect.fit_transform(str(x) for x in df['text'].values)\n",
    "y = df['class'].values\n",
    "\n",
    "\n",
    "a_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(a_train.shape)\n",
    "print (b_train.shape)\n",
    "print (a_test.shape)\n",
    "#print (b_test.shape)\n",
    "print (type(a_train))\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100) #, max_features=69)\n",
    "rf_classifier = rf_classifier.fit(a_train.toarray(), b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_train = MultinomialNB().fit(a_train.toarray(), b_train)\n",
    "pred_nb=clf_train.predict(a_test.toarray())\n",
    "prediction_nb=confusion_matrix(b_test,pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 0.84430176565\n",
      "\n",
      "precision: 0.856003739962\n",
      "\n",
      "recall: 0.84430176565\n",
      "\n",
      " confussion matrix:\n",
      " [[ 21  24  11]\n",
      " [  0 294  19]\n",
      " [  0  43 211]]\n",
      "\n",
      " clasification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       1.00      0.38      0.55        56\n",
      "    neutral       0.81      0.94      0.87       313\n",
      "   positive       0.88      0.83      0.85       254\n",
      "\n",
      "avg / total       0.86      0.84      0.83       623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import sklearn.metrics as metrics\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
    "#print ('\\nscore:', classifier.score(a_train, b_test))\n",
    "\n",
    "print ('\\naccuracy:', accuracy_score(b_test, pred_nb))\n",
    "print ('\\nprecision:', precision_score(b_test, pred_nb, average='weighted'))\n",
    "print ('\\nrecall:', recall_score(b_test, pred_nb, average = 'weighted'))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(b_test, pred_nb))\n",
    "print ('\\n clasification report:\\n', classification_report(b_test, pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy: 0.898876404494\n",
      "\n",
      "precision: 0.90046609342\n",
      "\n",
      "recall: 0.898876404494\n",
      "\n",
      " confussion matrix:\n",
      " [[ 37   8  11]\n",
      " [  0 299  14]\n",
      " [  2  28 224]]\n",
      "\n",
      " clasification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.95      0.66      0.78        56\n",
      "    neutral       0.89      0.96      0.92       313\n",
      "   positive       0.90      0.88      0.89       254\n",
      "\n",
      "avg / total       0.90      0.90      0.90       623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = rf_classifier.predict(a_test.toarray()) \n",
    "\n",
    "#import sklearn.metrics as metrics\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
    "#print ('\\nscore:', classifier.score(a_train, b_test))\n",
    "\n",
    "print ('\\naccuracy:', accuracy_score(b_test, prediction))\n",
    "print ('\\nprecision:', precision_score(b_test, prediction, average='weighted'))\n",
    "print ('\\nrecall:', recall_score(b_test, prediction, average = 'weighted'))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(b_test, prediction))\n",
    "print ('\\n clasification report:\\n', classification_report(b_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "5\n",
      "tweets [['case', 'study', 'azure', 'insofe'], ['insofe', 'reported', 'issues', 'azure'], ['azure', 'several', 'issues'], ['azure', 'losing', 'ground', 'aws'], ['azure', 'training', 'session', 'conducted', 'hyderabad']]\n",
      "predicted values  [u'neutral' u'neutral' u'neutral' u'neutral' u'positive']\n",
      "testlables are  ['positive', 'negative', 'negative', 'negative', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# Test the classifier\n",
    "testTweet = [\"@Microsoft has case study on Azure with insofe\",\"Insofe reported issues with Azure\", \n",
    "             \"Azure has several issues\", \"Azure is losing ground with AWS\", \n",
    "             \"Azure training session is being conducted in Hyderabad\"]\n",
    "\n",
    "testlabels = ['positive', 'negative', 'negative', 'negative', 'neutral']\n",
    "predicted=[]\n",
    "print (type(testTweet))\n",
    "print (len(testTweet))\n",
    "    \n",
    "df = pd.DataFrame(testTweet, columns=['text']) #[x for x in iter(testTweet)]\n",
    "#print (type (features))\n",
    "\n",
    "# Get tweet words\n",
    "tweets = []\n",
    "for i in range(len(df)):\n",
    "    tweet = df['text'][i]\n",
    "    processedTweet = processTweet(tweet)\n",
    "    #print(\"processedTweet\",processedTweet)\n",
    "    featureVector = getFeatureVector(processedTweet)\n",
    "    #print (\"featureVector\", featureVector)\n",
    "    tweets.append((featureVector));\n",
    "#end loop\n",
    "\n",
    "print (\"tweets\", tweets)\n",
    "\n",
    "\n",
    "X = tfidf_vect.transform([str(x) for x in tweets])\n",
    "predicted = rf_classifier.predict(X.toarray())\n",
    "\n",
    "\n",
    "print (\"predicted values \", predicted)\n",
    "print (\"testlables are \", testlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on some random text\n",
      "\n",
      "accuracy: 0.0\n",
      "\n",
      "precision: 0.0\n",
      "\n",
      "recall: 0.0\n",
      "\n",
      " confussion matrix:\n",
      " [[0 3 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "\n",
      " clasification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         1\n",
      "   positive       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.00      0.00      0.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import sklearn.metrics as metrics\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"on some random text\")\n",
    "print ('\\naccuracy:', accuracy_score(testlabels, predicted))\n",
    "print ('\\nprecision:', precision_score(testlabels, predicted, average='weighted'))\n",
    "print ('\\nrecall:', recall_score(testlabels, predicted, average = 'weighted'))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(testlabels, predicted))\n",
    "print ('\\n clasification report:\\n', classification_report(testlabels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
